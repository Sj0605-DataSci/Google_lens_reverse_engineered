{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageSimilaritySearch:\n",
    "    def __init__(self, dataset_path, method='resnet'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.method = method\n",
    "        self.image_paths = []\n",
    "        self.features = None\n",
    "        self.categories = ['bags', 'dress', 'pants', 'shorts', 'upperwear']\n",
    "        \n",
    "        # Initialize the chosen model\n",
    "        if method == 'resnet':\n",
    "            self.model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "        elif method == 'efficientnet':\n",
    "            self.model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "        elif method == 'clip':\n",
    "            self.model, self.preprocess = clip.load(\"ViT-B/32\", device=\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "            \n",
    "    def preprocess_image(self, img_path):\n",
    "        if self.method in ['resnet', 'efficientnet']:\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "            return x\n",
    "        elif self.method == 'clip':\n",
    "            img = Image.open(img_path)\n",
    "            return self.preprocess(img).unsqueeze(0)\n",
    "\n",
    "    def extract_features(self):\n",
    "        features = []\n",
    "        self.image_paths = []\n",
    "\n",
    "        for category in self.categories:\n",
    "            category_path = os.path.join(self.dataset_path, category)\n",
    "            if os.path.exists(category_path):\n",
    "                for img_name in tqdm(os.listdir(category_path)):\n",
    "                    img_path = os.path.join(category_path, img_name)\n",
    "                    try:\n",
    "                        if self.method in ['resnet', 'efficientnet']:\n",
    "                            img_tensor = self.preprocess_image(img_path)\n",
    "                            feature = self.model.predict(img_tensor, verbose=0)\n",
    "                            features.append(feature.flatten())\n",
    "                        elif self.method == 'clip':\n",
    "                            img_tensor = self.preprocess_image(img_path).to(\n",
    "                                \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "                            )\n",
    "                            with torch.no_grad():\n",
    "                                feature = self.model.encode_image(img_tensor)\n",
    "                                features.append(feature.cpu().numpy().flatten())\n",
    "                        \n",
    "                        self.image_paths.append(img_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "        self.features = np.array(features)\n",
    "        print(f\"Extracted features shape: {self.features.shape}\")\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.init_faiss_index()\n",
    "\n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved)\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        return {\n",
    "            'mean_precision': np.mean(precisions),\n",
    "            'mean_recall': np.mean(recalls),\n",
    "            'f1_score': 2 * np.mean(precisions) * np.mean(recalls) / (np.mean(precisions) + np.mean(recalls))\n",
    "        }    \n",
    "\n",
    "    def init_faiss_index(self):\n",
    "        # Normalize features for FAISS\n",
    "        self.features = self.features.astype('float32')\n",
    "        self.features_normalized = self.features / np.linalg.norm(self.features, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.features.shape[1])\n",
    "        self.index.add(self.features_normalized)\n",
    "\n",
    "    def find_similar_images(self, query_image_path, k=5):\n",
    "        # Extract features for query image\n",
    "        if self.method in ['resnet', 'efficientnet']:\n",
    "            query_tensor = self.preprocess_image(query_image_path)\n",
    "            query_features = self.model.predict(query_tensor, verbose=0)\n",
    "        elif self.method == 'clip':\n",
    "            query_tensor = self.preprocess_image(query_image_path).to(\n",
    "                \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                query_features = self.model.encode_image(query_tensor).cpu().numpy()\n",
    "\n",
    "        # Normalize query features\n",
    "        query_features = query_features.astype('float32')\n",
    "        query_features_normalized = query_features / np.linalg.norm(query_features)\n",
    "\n",
    "        # Search using FAISS\n",
    "        D, I = self.index.search(query_features_normalized.reshape(1, -1), k)\n",
    "        \n",
    "        similar_images = [self.image_paths[idx] for idx in I[0]]\n",
    "        similarities = D[0]\n",
    "        \n",
    "        return list(zip(similar_images, similarities))\n",
    "\n",
    "# Siamese Network Implementation\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.feature_extractor = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
    "        self.fc = nn.Linear(512, 128)\n",
    "        self.image_paths = []\n",
    "        self.features = None\n",
    "        self.device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        return out1, out2\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    \n",
    "    def extract_features(self, dataset_path):\n",
    "        self.image_paths = []\n",
    "        features = []\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        \n",
    "        categories = ['bags', 'dress', 'pants', 'shorts', 'upperwear']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for category in categories:\n",
    "                category_path = os.path.join(dataset_path, category)\n",
    "                if os.path.exists(category_path):\n",
    "                    for img_name in tqdm(os.listdir(category_path)):\n",
    "                        try:\n",
    "                            img_path = os.path.join(category_path, img_name)\n",
    "                            img_tensor = self.preprocess_image(img_path).to(self.device)\n",
    "                            # Extract features using forward_one\n",
    "                            feature = self.forward_one(img_tensor)\n",
    "                            features.append(feature.cpu().numpy().flatten())\n",
    "                            self.image_paths.append(img_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        \n",
    "        self.features = np.array(features)\n",
    "        print(f\"Extracted features shape: {self.features.shape}\")\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.init_faiss_index()\n",
    "    \n",
    "    def init_faiss_index(self):\n",
    "        # Normalize features for FAISS\n",
    "        self.features = self.features.astype('float32')\n",
    "        self.features_normalized = self.features / np.linalg.norm(self.features, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.features.shape[1])\n",
    "        self.index.add(self.features_normalized)\n",
    "    \n",
    "    def find_similar_images(self, query_image_path, k=5):\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess query image\n",
    "            query_tensor = self.preprocess_image(query_image_path).to(self.device)\n",
    "            query_features = self.forward_one(query_tensor)\n",
    "            query_features = query_features.cpu().numpy()\n",
    "\n",
    "        # Normalize query features\n",
    "        query_features = query_features.astype('float32')\n",
    "        query_features_normalized = query_features / np.linalg.norm(query_features)\n",
    "\n",
    "        # Search using FAISS\n",
    "        D, I = self.index.search(query_features_normalized.reshape(1, -1), k)\n",
    "        \n",
    "        similar_images = [self.image_paths[idx] for idx in I[0]]\n",
    "        similarities = D[0]\n",
    "        \n",
    "        return list(zip(similar_images, similarities))\n",
    "    \n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved) if retrieved else 0\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set) if true_set else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        f1_score = 2 * mean_precision * mean_recall / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'mean_precision': mean_precision,\n",
    "            'mean_recall': mean_recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "    \n",
    "    def compute_similarity(self, img1_path, img2_path):\n",
    "        \"\"\"Compute similarity between two images using the siamese network\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            img1_tensor = self.preprocess_image(img1_path).to(self.device)\n",
    "            img2_tensor = self.preprocess_image(img2_path).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            emb1, emb2 = self.forward(img1_tensor, img2_tensor)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
    "            \n",
    "            return similarity.item()\n",
    "    \n",
    "    def train_step(self, img1, img2, label, criterion, optimizer):\n",
    "        \"\"\"Single training step for siamese network\"\"\"\n",
    "        # Forward pass\n",
    "        output1, output2 = self.forward(img1, img2)\n",
    "        \n",
    "        # Calculate contrastive loss\n",
    "        loss = criterion(output1, output2, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "\n",
    "class AutoencoderSearch(nn.Module):\n",
    "    def __init__(self, input_size=224*224*3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.image_paths = []\n",
    "        self.features = None\n",
    "        self.device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    \n",
    "    def extract_features(self, dataset_path):\n",
    "        self.image_paths = []\n",
    "        features = []\n",
    "        self.eval()\n",
    "        \n",
    "        categories = ['bags', 'dress', 'pants', 'shorts', 'upperwear']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for category in categories:\n",
    "                category_path = os.path.join(dataset_path, category)\n",
    "                if os.path.exists(category_path):\n",
    "                    for img_name in tqdm(os.listdir(category_path)):\n",
    "                        try:\n",
    "                            img_path = os.path.join(category_path, img_name)\n",
    "                            img_tensor = self.preprocess_image(img_path).to(self.device)\n",
    "                            img_tensor = img_tensor.view(img_tensor.size(0), -1)\n",
    "                            encoded, _ = self.forward(img_tensor)\n",
    "                            features.append(encoded.cpu().numpy().flatten())\n",
    "                            self.image_paths.append(img_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        \n",
    "        self.features = np.array(features)\n",
    "        print(f\"Extracted features shape: {self.features.shape}\")\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.init_faiss_index()\n",
    "    \n",
    "    def init_faiss_index(self):\n",
    "        self.features = self.features.astype('float32')\n",
    "        self.features_normalized = self.features / np.linalg.norm(self.features, axis=1)[:, np.newaxis]\n",
    "        self.index = faiss.IndexFlatIP(self.features.shape[1])\n",
    "        self.index.add(self.features_normalized)\n",
    "    \n",
    "    def find_similar_images(self, query_image_path, k=5):\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_tensor = self.preprocess_image(query_image_path).to(self.device)\n",
    "            query_tensor = query_tensor.view(query_tensor.size(0), -1)\n",
    "            query_features, _ = self.forward(query_tensor)\n",
    "            query_features = query_features.cpu().numpy()\n",
    "        \n",
    "        # Normalize query features\n",
    "        query_features = query_features.astype('float32')\n",
    "        query_features_normalized = query_features / np.linalg.norm(query_features)\n",
    "        \n",
    "        # Search using FAISS\n",
    "        D, I = self.index.search(query_features_normalized.reshape(1, -1), k)\n",
    "        \n",
    "        similar_images = [self.image_paths[idx] for idx in I[0]]\n",
    "        similarities = D[0]\n",
    "        \n",
    "        return list(zip(similar_images, similarities))\n",
    "    \n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved) if retrieved else 0\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set) if true_set else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        f1_score = 2 * mean_precision * mean_recall / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'mean_precision': mean_precision,\n",
    "            'mean_recall': mean_recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "class LSHSearch:\n",
    "    def __init__(self, feature_dimension, num_tables=10, num_bits=8):\n",
    "        self.num_tables = num_tables\n",
    "        self.num_bits = num_bits\n",
    "        self.tables = []\n",
    "        self.random_vectors = []\n",
    "        self.image_paths = []\n",
    "        self.features = None\n",
    "        self.device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        for _ in range(num_tables):\n",
    "            random_vec = np.random.randn(num_bits, feature_dimension)\n",
    "            self.random_vectors.append(random_vec)\n",
    "            self.tables.append([])  # Changed from dict to list\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    \n",
    "    def _hash_vector(self, vector, random_vec):\n",
    "        projections = np.dot(random_vec, vector)\n",
    "        return ''.join(['1' if p > 0 else '0' for p in projections])\n",
    "    \n",
    "    def index(self, features, image_paths):\n",
    "        self.features = features\n",
    "        self.image_paths = image_paths\n",
    "        self.hash_tables = [{} for _ in range(self.num_tables)]\n",
    "        \n",
    "        for idx, feature in enumerate(features):\n",
    "            for i, random_vec in enumerate(self.random_vectors):\n",
    "                hash_key = self._hash_vector(feature, random_vec)\n",
    "                if hash_key not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][hash_key] = []\n",
    "                self.hash_tables[i][hash_key].append(idx)\n",
    "    \n",
    "    def extract_features(self, dataset_path):\n",
    "        self.image_paths = []\n",
    "        features = []\n",
    "        categories = ['bags', 'dress', 'pants', 'shorts', 'upperwear']\n",
    "        \n",
    "        resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(self.device)\n",
    "        feature_extractor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for category in categories:\n",
    "                category_path = os.path.join(dataset_path, category)\n",
    "                if os.path.exists(category_path):\n",
    "                    for img_name in tqdm(os.listdir(category_path)):\n",
    "                        try:\n",
    "                            img_path = os.path.join(category_path, img_name)\n",
    "                            img_tensor = self.preprocess_image(img_path).to(self.device)\n",
    "                            feature = feature_extractor(img_tensor)\n",
    "                            features.append(feature.cpu().numpy().flatten())\n",
    "                            self.image_paths.append(img_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        \n",
    "        self.features = np.array(features)\n",
    "        print(f\"Extracted features shape: {self.features.shape}\")\n",
    "        self.index(self.features, self.image_paths)\n",
    "    \n",
    "    def find_similar_images(self, query_image_path, k=5):\n",
    "        # Extract features for query image\n",
    "        resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(self.device)\n",
    "        feature_extractor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            query_tensor = self.preprocess_image(query_image_path).to(self.device)\n",
    "            query_features = feature_extractor(query_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Find similar images using LSH\n",
    "        candidate_indices = set()\n",
    "        for i, random_vec in enumerate(self.random_vectors):\n",
    "            hash_key = self._hash_vector(query_features, random_vec)\n",
    "            if hash_key in self.hash_tables[i]:\n",
    "                candidate_indices.update(self.hash_tables[i][hash_key])\n",
    "        \n",
    "        # Calculate actual distances for candidates\n",
    "        distances = []\n",
    "        for idx in candidate_indices:\n",
    "            feature = self.features[idx]\n",
    "            distance = np.dot(query_features, feature) / (np.linalg.norm(query_features) * np.linalg.norm(feature))\n",
    "            distances.append((self.image_paths[idx], distance))\n",
    "        \n",
    "        # If no candidates found, return k random images\n",
    "        if not distances:\n",
    "            random_indices = np.random.choice(len(self.image_paths), k, replace=False)\n",
    "            distances = [(self.image_paths[idx], 0.0) for idx in random_indices]\n",
    "        \n",
    "        # Return top k results\n",
    "        return sorted(distances, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved) if retrieved else 0\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set) if true_set else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        f1_score = 2 * mean_precision * mean_recall / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'mean_precision': mean_precision,\n",
    "            'mean_recall': mean_recall,\n",
    "            'f1_score': f1_score\n",
    "        }            \n",
    "\n",
    "class TripletNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torch.hub.load('pytorch/vision:v0.10.0', \n",
    "                                              'resnet18', pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])\n",
    "        self.fc = nn.Linear(512, 128)\n",
    "        self.image_paths = []\n",
    "        self.features = None\n",
    "        self.device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_out = self.forward_one(anchor)\n",
    "        positive_out = self.forward_one(positive)\n",
    "        negative_out = self.forward_one(negative)\n",
    "        return anchor_out, positive_out, negative_out\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return transform(img).unsqueeze(0)\n",
    "    \n",
    "    def extract_features(self, dataset_path):\n",
    "        self.image_paths = []\n",
    "        features = []\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        \n",
    "        categories = ['bags', 'dress', 'pants', 'shorts', 'upperwear']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for category in categories:\n",
    "                category_path = os.path.join(dataset_path, category)\n",
    "                if os.path.exists(category_path):\n",
    "                    for img_name in tqdm(os.listdir(category_path)):\n",
    "                        try:\n",
    "                            img_path = os.path.join(category_path, img_name)\n",
    "                            img_tensor = self.preprocess_image(img_path).to(self.device)\n",
    "                            feature = self.forward_one(img_tensor)\n",
    "                            features.append(feature.cpu().numpy().flatten())\n",
    "                            self.image_paths.append(img_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "        \n",
    "        self.features = np.array(features)\n",
    "        print(f\"Extracted features shape: {self.features.shape}\")\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.init_faiss_index()\n",
    "    \n",
    "    def init_faiss_index(self):\n",
    "        # Normalize features for FAISS\n",
    "        self.features = self.features.astype('float32')\n",
    "        self.features_normalized = self.features / np.linalg.norm(self.features, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.features.shape[1])\n",
    "        self.index.add(self.features_normalized)\n",
    "    \n",
    "    def find_similar_images(self, query_image_path, k=5):\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess query image\n",
    "            query_tensor = self.preprocess_image(query_image_path).to(self.device)\n",
    "            query_features = self.forward_one(query_tensor)\n",
    "            query_features = query_features.cpu().numpy()\n",
    "\n",
    "        # Normalize query features\n",
    "        query_features = query_features.astype('float32')\n",
    "        query_features_normalized = query_features / np.linalg.norm(query_features)\n",
    "\n",
    "        # Search using FAISS\n",
    "        D, I = self.index.search(query_features_normalized.reshape(1, -1), k)\n",
    "        \n",
    "        similar_images = [self.image_paths[idx] for idx in I[0]]\n",
    "        similarities = D[0]\n",
    "        \n",
    "        return list(zip(similar_images, similarities))\n",
    "    \n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved) if retrieved else 0\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set) if true_set else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        f1_score = 2 * mean_precision * mean_recall / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'mean_precision': mean_precision,\n",
    "            'mean_recall': mean_recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, models_list=None, weights=None):\n",
    "        # Default weights based on typical performance characteristics\n",
    "        self.default_weights = {\n",
    "    'clip': 0.25,      # CLIP for semantic understanding\n",
    "    'resnet': 0.20,    # ResNet for general features\n",
    "    'efficientnet': 0.15,  # EfficientNet for efficiency\n",
    "    'autoencoder': 0.15,   # Autoencoder for latent space representation\n",
    "    'siamese': 0.10,    # Siamese for paired similarity\n",
    "    'triplet': 0.10,    # Triplet for relative similarity\n",
    "    'lsh': 0.05        # LSH for fast approximate search\n",
    "}\n",
    "        \n",
    "        self.models = models_list\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            # Assign weights based on which models are actually provided\n",
    "            total_weight = sum(self.default_weights[model.method] \n",
    "                             for model in models_list \n",
    "                             if hasattr(model, 'method') and model.method in self.default_weights)\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            self.weights = [\n",
    "                self.default_weights[model.method] / total_weight \n",
    "                if hasattr(model, 'method') and model.method in self.default_weights \n",
    "                else 1.0 / len(models_list) \n",
    "                for model in models_list\n",
    "            ]\n",
    "\n",
    "    def _combine_results(self, all_results, k):\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Weighted voting with normalized scores\n",
    "        for (results, weight) in all_results:\n",
    "            # Normalize scores within each model's results\n",
    "            max_score = max(score for _, score in results)\n",
    "            min_score = min(score for _, score in results)\n",
    "            score_range = max_score - min_score if max_score != min_score else 1\n",
    "            \n",
    "            for path, score in results:\n",
    "                if path not in combined_scores:\n",
    "                    combined_scores[path] = 0\n",
    "                # Normalize score to [0,1] range before applying weight\n",
    "                normalized_score = (score - min_score) / score_range\n",
    "                combined_scores[path] += normalized_score * weight\n",
    "        \n",
    "        # Sort and return top k results\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return sorted_results\n",
    "\n",
    "    def find_similar_images(self, query_image, k=5):\n",
    "        all_results = []\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            try:\n",
    "                results = model.find_similar_images(query_image, k)\n",
    "                all_results.append((results, weight))\n",
    "            except Exception as e:\n",
    "                print(f\"Error with model {model.__class__.__name__}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self._combine_results(all_results, k)\n",
    "\n",
    "    def evaluate_retrieval(self, test_queries, ground_truth):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for query, true_matches in zip(test_queries, ground_truth):\n",
    "            results = self.find_similar_images(query)\n",
    "            retrieved = set([path for path, _ in results])\n",
    "            true_set = set(true_matches)\n",
    "            \n",
    "            precision = len(retrieved.intersection(true_set)) / len(retrieved) if retrieved else 0\n",
    "            recall = len(retrieved.intersection(true_set)) / len(true_set) if true_set else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        f1_score = 2 * mean_precision * mean_recall / (mean_precision + mean_recall) if (mean_precision + mean_recall) > 0 else 0  \n",
    "        return {\n",
    "            'mean_precision': mean_precision,\n",
    "            'mean_recall': mean_recall,\n",
    "            'f1_score': f1_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def test_similarity_search():\n",
    "    dataset_path = \"/Users/sammy/Desktop/Shoppin/sampled_fashion_dataset/\"\n",
    "    results_data = []\n",
    "    \n",
    "    # Initialize all models\n",
    "    print(\"Initializing models...\")\n",
    "    models = {\n",
    "        'resnet': ImageSimilaritySearch(dataset_path, method='resnet'),\n",
    "        'clip': ImageSimilaritySearch(dataset_path, method='clip'),\n",
    "        'efficientnet': ImageSimilaritySearch(dataset_path, method='efficientnet'),\n",
    "        'autoencoder': AutoencoderSearch(),\n",
    "        'siamese': SiameseNetwork(),\n",
    "        'triplet': TripletNetwork(),\n",
    "        'lsh': LSHSearch(feature_dimension=512)  # 512 for ResNet18 features\n",
    "    }\n",
    "    \n",
    "    # Extract features for all models\n",
    "    print(\"\\nExtracting features for all models...\")\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        if name in ['resnet', 'clip', 'efficientnet']:\n",
    "            model.extract_features()\n",
    "        else:\n",
    "            model.extract_features(dataset_path)  # For other models that need dataset_path\n",
    "        extraction_time = time.time() - start_time\n",
    "        print(f\"{name.upper()} feature extraction time: {extraction_time:.2f} seconds\")\n",
    "    \n",
    "    # Create test queries and ground truth\n",
    "    test_queries = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    # Get test images from each category\n",
    "    print(\"\\nPreparing test images...\")\n",
    "    for category in models['resnet'].categories:\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        if os.path.exists(category_path):\n",
    "            category_images = [os.path.join(category_path, img) \n",
    "                             for img in os.listdir(category_path)]\n",
    "            \n",
    "            # Select 10 random images for testing from each category\n",
    "            if len(category_images) > 10:\n",
    "                test_images = random.sample(category_images, 10)\n",
    "                for test_img in test_images:\n",
    "                    test_queries.append(test_img)\n",
    "                    truth = [img for img in category_images if img != test_img]\n",
    "                    ground_truth.append(truth)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    print(\"\\nEvaluating all models...\")\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name.upper()}:\")\n",
    "        \n",
    "        # Time the evaluation\n",
    "        start_time = time.time()\n",
    "        if hasattr(model, 'evaluate_retrieval'):\n",
    "            metrics = model.evaluate_retrieval(test_queries, ground_truth)\n",
    "            eval_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Metrics for {model_name}:\")\n",
    "            print(f\"Mean Precision: {metrics['mean_precision']:.4f}\")\n",
    "            print(f\"Mean Recall: {metrics['mean_recall']:.4f}\")\n",
    "            print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "            print(f\"Evaluation Time: {eval_time:.2f} seconds\")\n",
    "            \n",
    "            # Store results\n",
    "            results_data.append({\n",
    "                'model': model_name,\n",
    "                'precision': metrics['mean_precision'],\n",
    "                'recall': metrics['mean_recall'],\n",
    "                'f1_score': metrics['f1_score'],\n",
    "                'eval_time': eval_time\n",
    "            })\n",
    "    \n",
    "    # Test hybrid approach\n",
    "    print(\"\\nTesting Hybrid Approach...\")\n",
    "    hybrid_model = HybridSearch([\n",
    "        models['resnet'], \n",
    "        models['clip'],\n",
    "        models['efficientnet'],\n",
    "        models['autoencoder'],\n",
    "        models['siamese'],\n",
    "        models['triplet'],\n",
    "        models['lsh']\n",
    "    ])\n",
    "    start_time = time.time()\n",
    "    hybrid_metrics = hybrid_model.evaluate_retrieval(test_queries[:10], ground_truth[:10])\n",
    "    hybrid_time = time.time() - start_time\n",
    "    \n",
    "    results_data.append({\n",
    "        'model': 'hybrid',\n",
    "        'precision': hybrid_metrics['mean_precision'],\n",
    "        'recall': hybrid_metrics['mean_recall'],\n",
    "        'f1_score': hybrid_metrics['f1_score'],\n",
    "        'eval_time': hybrid_time\n",
    "    })\n",
    "    \n",
    "    # Create detailed performance report\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    print(\"\\nDetailed Performance Report:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f'model_comparison_results_{timestamp}.csv', index=False)\n",
    "\n",
    "    detailed_metrics_data = []\n",
    "    for idx, row in results_df.iterrows():\n",
    "        metrics_entry = {\n",
    "            'Model': row['model'],\n",
    "            'Mean Precision': f\"{row['precision']:.4f}\",\n",
    "            'Mean Recall': f\"{row['recall']:.4f}\",\n",
    "            'F1 Score': f\"{row['f1_score']:.4f}\",\n",
    "            'Evaluation Time (s)': f\"{row['eval_time']:.2f}\"\n",
    "        }\n",
    "        detailed_metrics_data.append(metrics_entry)\n",
    "    \n",
    "    detailed_metrics_df = pd.DataFrame(detailed_metrics_data)\n",
    "\n",
    "    query_results_data = []\n",
    "    \n",
    "    # Test individual queries with detailed logging\n",
    "    print(\"\\nTesting individual query examples...\")\n",
    "    test_images = random.sample(test_queries, 5)  # Select 5 random test images\n",
    "    \n",
    "    for idx, test_image in enumerate(test_images, 1):\n",
    "        query_category = test_image.split('/')[-2]  # Extract category from path\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if hasattr(model, 'find_similar_images'):\n",
    "                start_time = time.time()\n",
    "                results = model.find_similar_images(test_image)\n",
    "                query_time = time.time() - start_time\n",
    "                \n",
    "                # Log each retrieved image\n",
    "                for rank, (path, similarity) in enumerate(results, 1):\n",
    "                    retrieved_category = path.split('/')[-2]\n",
    "                    is_correct = retrieved_category == query_category\n",
    "                    \n",
    "                    query_results_data.append({\n",
    "                        'Query_ID': idx,\n",
    "                        'Query_Image': test_image,\n",
    "                        'Query_Category': query_category,\n",
    "                        'Model': model_name,\n",
    "                        'Retrieved_Image': path,\n",
    "                        'Retrieved_Category': retrieved_category,\n",
    "                        'Rank': rank,\n",
    "                        'Similarity_Score': similarity,\n",
    "                        'Is_Correct_Category': is_correct,\n",
    "                        'Query_Time': query_time\n",
    "                    })\n",
    "    \n",
    "    query_results_df = pd.DataFrame(query_results_data)\n",
    "    \n",
    "    # Save both DataFrames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    detailed_metrics_df.to_csv(f'detailed_metrics_{timestamp}.csv', index=False)\n",
    "    query_results_df.to_csv(f'query_results_{timestamp}.csv', index=False)\n",
    "    \n",
    "    # Print summary tables\n",
    "    print(\"\\n=== Detailed Model Performance Metrics ===\")\n",
    "    print(detailed_metrics_df.to_string())\n",
    "    \n",
    "    print(\"\\n=== Query Results Analysis ===\")\n",
    "    summary = query_results_df.groupby('Model').agg({\n",
    "        'Is_Correct_Category': 'mean',\n",
    "        'Query_Time': 'mean',\n",
    "        'Similarity_Score': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    print(summary)\n",
    "    \n",
    "    # Create model architecture details\n",
    "    model_details = {\n",
    "        'resnet': 'ResNet50 (ImageNet pretrained)',\n",
    "        'clip': 'CLIP ViT-B/32',\n",
    "        'efficientnet': 'EfficientNetB0',\n",
    "        'autoencoder': 'Custom AE (32-dim latent)',\n",
    "        'siamese': 'Siamese ResNet18',\n",
    "        'triplet': 'Triplet ResNet18',\n",
    "        'lsh': 'LSH (512-dim, 10 tables)',\n",
    "        'hybrid': 'Weighted Ensemble'\n",
    "    }\n",
    "    \n",
    "    feature_dimensions = {\n",
    "        'resnet': 2048,\n",
    "        'clip': 512,\n",
    "        'efficientnet': 1280,\n",
    "        'autoencoder': 32,\n",
    "        'siamese': 128,\n",
    "        'triplet': 128,\n",
    "        'lsh': 512,\n",
    "        'hybrid': 'Variable'\n",
    "    }\n",
    "    \n",
    "    # Update detailed metrics with architecture info\n",
    "    detailed_metrics_df['Model Architecture'] = detailed_metrics_df['Model'].map(model_details)\n",
    "    detailed_metrics_df['Feature Dimension'] = detailed_metrics_df['Model'].map(feature_dimensions)\n",
    "    \n",
    "    return detailed_metrics_df, query_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features for all models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.44it/s]\n",
      "100%|██████████| 100/100 [00:09<00:00, 11.05it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.83it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.54it/s]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 2048)\n",
      "RESNET feature extraction time: 45.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.41it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 38.36it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 40.16it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 37.65it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 512)\n",
      "CLIP feature extraction time: 13.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 14.19it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.37it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.28it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.89it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 1280)\n",
      "EFFICIENTNET feature extraction time: 32.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.00it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 47.89it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 52.29it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 48.41it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 49.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 32)\n",
      "AUTOENCODER feature extraction time: 10.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.57it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.04it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.64it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 62.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 128)\n",
      "SIAMESE feature extraction time: 7.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 64.70it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.41it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 68.22it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.68it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.10it/s]\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 128)\n",
      "TRIPLET feature extraction time: 7.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 62.91it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.50it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.70it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.16it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (500, 512)\n",
      "LSH feature extraction time: 8.10 seconds\n",
      "\n",
      "Preparing test images...\n",
      "\n",
      "Evaluating all models...\n",
      "\n",
      "Evaluating RESNET:\n",
      "Metrics for resnet:\n",
      "Mean Precision: 0.7000\n",
      "Mean Recall: 0.0354\n",
      "F1 Score: 0.0673\n",
      "Evaluation Time: 4.56 seconds\n",
      "\n",
      "Evaluating CLIP:\n",
      "Metrics for clip:\n",
      "Mean Precision: 0.6480\n",
      "Mean Recall: 0.0327\n",
      "F1 Score: 0.0623\n",
      "Evaluation Time: 1.30 seconds\n",
      "\n",
      "Evaluating EFFICIENTNET:\n",
      "Metrics for efficientnet:\n",
      "Mean Precision: 0.6400\n",
      "Mean Recall: 0.0323\n",
      "F1 Score: 0.0615\n",
      "Evaluation Time: 3.03 seconds\n",
      "\n",
      "Evaluating AUTOENCODER:\n",
      "Metrics for autoencoder:\n",
      "Mean Precision: 0.5160\n",
      "Mean Recall: 0.0261\n",
      "F1 Score: 0.0496\n",
      "Evaluation Time: 1.01 seconds\n",
      "\n",
      "Evaluating SIAMESE:\n",
      "Metrics for siamese:\n",
      "Mean Precision: 0.6560\n",
      "Mean Recall: 0.0331\n",
      "F1 Score: 0.0631\n",
      "Evaluation Time: 0.81 seconds\n",
      "\n",
      "Evaluating TRIPLET:\n",
      "Metrics for triplet:\n",
      "Mean Precision: 0.6720\n",
      "Mean Recall: 0.0339\n",
      "F1 Score: 0.0646\n",
      "Evaluation Time: 0.77 seconds\n",
      "\n",
      "Evaluating LSH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for lsh:\n",
      "Mean Precision: 0.6800\n",
      "Mean Recall: 0.0343\n",
      "F1 Score: 0.0654\n",
      "Evaluation Time: 10.84 seconds\n",
      "\n",
      "Testing Hybrid Approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Performance Report:\n",
      "          model  precision    recall  f1_score  eval_time\n",
      "0        resnet      0.700  0.035354  0.067308   4.560920\n",
      "1          clip      0.648  0.032727  0.062308   1.298262\n",
      "2  efficientnet      0.640  0.032323  0.061538   3.026808\n",
      "3   autoencoder      0.516  0.026061  0.049615   1.014230\n",
      "4       siamese      0.656  0.033131  0.063077   0.807374\n",
      "5       triplet      0.672  0.033939  0.064615   0.768623\n",
      "6           lsh      0.680  0.034343  0.065385  10.840207\n",
      "7        hybrid      0.740  0.037374  0.071154   4.586196\n",
      "\n",
      "Testing individual query examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/sammy/Desktop/Shoppin/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sammy/Desktop/Shoppin/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/sammy/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed Model Performance Metrics ===\n",
      "          Model Mean Precision Mean Recall F1 Score Evaluation Time (s) Average Query Time (s) Memory Usage (MB) Model Architecture Feature Dimension\n",
      "0        resnet         0.7000      0.0354   0.0673                4.56                   None              None               None              None\n",
      "1          clip         0.6480      0.0327   0.0623                1.30                   None              None               None              None\n",
      "2  efficientnet         0.6400      0.0323   0.0615                3.03                   None              None               None              None\n",
      "3   autoencoder         0.5160      0.0261   0.0496                1.01                   None              None               None              None\n",
      "4       siamese         0.6560      0.0331   0.0631                0.81                   None              None               None              None\n",
      "5       triplet         0.6720      0.0339   0.0646                0.77                   None              None               None              None\n",
      "6           lsh         0.6800      0.0343   0.0654               10.84                   None              None               None              None\n",
      "7        hybrid         0.7400      0.0374   0.0712                4.59                   None              None               None              None\n",
      "\n",
      "=== Query Results Analysis ===\n",
      "             Is_Correct_Category Query_Time Similarity_Score        \n",
      "                            mean       mean             mean     std\n",
      "Model                                                               \n",
      "autoencoder                 0.84     0.0202           0.9897  0.0102\n",
      "clip                        0.88     0.0287           0.9411  0.0426\n",
      "efficientnet                0.80     0.0641           0.7920  0.2005\n",
      "lsh                         1.00     0.2184           0.8827  0.0818\n",
      "resnet                      0.88     0.0940           0.8216  0.1258\n",
      "siamese                     0.92     0.0150           0.9123  0.0614\n",
      "triplet                     1.00     0.0149           0.8787  0.0826\n"
     ]
    }
   ],
   "source": [
    "metrics_df, results_df = test_similarity_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
